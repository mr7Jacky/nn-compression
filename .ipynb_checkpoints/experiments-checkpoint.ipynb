{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4addded",
   "metadata": {},
   "source": [
    "# Pruning Experiments\n",
    "\n",
    "The Goal of this notebook is to simplify the code down to exactly what we want to work with, rather than looking at an entire benchmark framework where most of the code and analysis is wasted because we are trying to make it do something it's not meant to do. The code found in this notebook is heavily influenced by fasterai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604d131",
   "metadata": {},
   "source": [
    "## Imports and Setting up Data\n",
    "\n",
    "Below are the libraries and modules required for most of the cells as well as some basic blocks for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95df9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Magic Commands\n",
    "\n",
    "# Auto reload modules as changes are made\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a56fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasterai\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "os.environ['TORCH_HOME'] = \"./models\"\n",
    "\n",
    "from utils import dataset_builder, accuracy, correct\n",
    "\n",
    "from tqdm import tqdm\n",
    "from online import OnlineStats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb15339",
   "metadata": {},
   "source": [
    "## Loading Data: Cifar 10\n",
    "\n",
    "Initially, we are looking at the CIFAR-10 Dataset to examine how ResNet-18 Architecture is affected by pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f096bc1",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d07eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIFAR10(train=True, download=False):\n",
    "    \"\"\"Thin wrapper around torchvision.datasets.CIFAR10\"\"\"\n",
    "    mean, std = [0.491, 0.482, 0.447], [0.247, 0.243, 0.262]\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "    if train:\n",
    "        preproc = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4)]\n",
    "    else:\n",
    "        preproc = []\n",
    "    dataset = dataset_builder('CIFAR10', train, normalize, preproc, download)\n",
    "    dataset.shape = (3, 32, 32)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976851b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar_10_train = CIFAR10(train=True, download=True)\n",
    "cifar_10_test = CIFAR10(train=False, download=True)\n",
    "\n",
    "cifar_10_train_dl = DataLoader(cifar_10_train, batch_size=128, num_workers=4, shuffle=True)\n",
    "cifar_10_test_dl = DataLoader(cifar_10_test, batch_size=128, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58fce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e76ae140",
   "metadata": {},
   "source": [
    "#### Importing the Models\n",
    "\n",
    "We are using the pretrained model from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254a4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## From PyTorch (Not Trained for Cifar10)\n",
    "#resnet18 = models.resnet18().to(device)\n",
    "\n",
    "## From Trained models \n",
    "resnet18 = torch.load(\"./saved/model/ResNet_10.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa4e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device cuda\n",
      "ResNet18: 11,689,512 parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Running on device {device}\")\n",
    "print(f\"ResNet18: {count_parameters(resnet18):,} parameters\\n\")\n",
    "\n",
    "# print(f\"Running on device {device}\")\n",
    "# print(f\"VGG16: {count_parameters(vgg16):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fully connected normalized weight criteria (per neuron)\n",
    "class FullNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.fc2.is_classifier = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "## Random Data\n",
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "my_nn = FullNet()\n",
    "result = my_nn(random_data)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff83c8",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3619bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c91afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs=5, criterion=nn.CrossEntropyLoss(), optimizer=optim.SGD(resnet18.parameters(), lr=0.0005, momentum=0.9, weight_decay=5e-4)):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        trainloader = tqdm(cifar_10_train_dl)\n",
    "        trainloader.set_description(f\"Train Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        total_loss = OnlineStats()\n",
    "        acc1 = OnlineStats()\n",
    "        acc5 = OnlineStats()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = resnet18(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ## Print Statistics\n",
    "            c1, c5 = correct(outputs, labels, (1, 5))\n",
    "            acc1.add(c1 / cifar_10_train_dl.batch_size)\n",
    "            acc5.add(c5 / cifar_10_train_dl.batch_size)\n",
    "            total_loss.add(loss.item() / cifar_10_train_dl.batch_size)\n",
    "\n",
    "            trainloader.set_postfix(loss=total_loss.mean, top1=acc1.mean, top5=acc5.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99401c9",
   "metadata": {},
   "source": [
    "#### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e167b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_model(model):\n",
    "    current_model_num = len([name for name in os.listdir('./saved/model/.')]) + 1\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    torch.save(model, f\"./saved/model/{model_name}_{current_model_num}.pt\")\n",
    "    torch.save(model.state_dict(), f\"./saved/state/{model_name}.pt_{current_model_num}\")\n",
    "\n",
    "    print(f\"Saved Model Version {current_model_num}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30acb4",
   "metadata": {},
   "source": [
    "### Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b6c13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 Accuracy: [0.7484]\n"
     ]
    }
   ],
   "source": [
    "# Get the accuracy of the model\n",
    "print(f\"ResNet18 Accuracy: {accuracy(resnet18, cifar_10_test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf4037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c390a764",
   "metadata": {},
   "source": [
    "### Looking at Fasterai Pruning\n",
    "\n",
    "Using the Sparsifier class allows us to specify a model, the granularity for which to prune, the method of pruning, and also the criteria used to calculate the 'importance' of each connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc67bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastcore.basics import store_attr\n",
    "from fasterai.sparse.criteria import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2eda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsifier():\n",
    "\n",
    "    def __init__(self, model, granularity, method, criteria):\n",
    "        store_attr()\n",
    "        self._save_weights() # Save the original weights\n",
    "\n",
    "    def prune_layer(self, module, sparsity):\n",
    "        weight = self.criteria(module, self.granularity)\n",
    "        mask = self._compute_mask(self.model, weight, sparsity)\n",
    "        module.register_buffer(\"_mask\", mask) # Put the mask into a buffer\n",
    "        self._apply(module)\n",
    "\n",
    "    def prune_model(self, sparsity):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                self.prune_layer(m, sparsity)\n",
    "\n",
    "    def _apply(self, module):\n",
    "        mask = getattr(module, \"_mask\")\n",
    "        module.weight.data.mul_(mask)\n",
    "\n",
    "        if self.granularity == 'filter': # If we remove complete filters, we want to remove the bias as well\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.mul_(mask.squeeze())\n",
    "\n",
    "    def _mask_grad(self):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Conv2d) and hasattr(m, '_mask'):\n",
    "                mask = getattr(m, \"_mask\")\n",
    "                if m.weight.grad is not None: # In case some layers are freezed\n",
    "                    m.weight.grad.mul_(mask)\n",
    "\n",
    "                if self.granularity == 'filter': # If we remove complete filters, we want to remove the bias as well\n",
    "                        if m.bias.grad is not None: # In case some layers are freezed\n",
    "                            m.bias.grad.mul_(mask.squeeze())\n",
    "\n",
    "    def _reset_weights(self):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init_weights = getattr(m, \"_init_weights\")\n",
    "                m.weight.data = init_weights.clone()\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init_weights = getattr(m, \"_init_weights\")\n",
    "                m.weight.data = init_weights.clone()\n",
    "                self._apply(m) # Reset the weights and apply the current mask\n",
    "\n",
    "    def _save_weights(self):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "\n",
    "    def _clean_buffers(self):\n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, nn.Conv2d) and hasattr(m, '_mask'):\n",
    "                del m._buffers[\"_mask\"]\n",
    "\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                del m._buffers[\"_init_weights\"]\n",
    "\n",
    "\n",
    "    def _compute_mask(self, model, weight, sparsity):\n",
    "        if self.method == 'global':\n",
    "            global_weight = torch.cat([self.criteria(m, self.granularity).view(-1) for m in model.modules() if isinstance(m, nn.Conv2d)])\n",
    "            threshold = torch.quantile(global_weight, sparsity/100) # Compute the threshold globally\n",
    "\n",
    "        elif self.method == 'local':\n",
    "            threshold = torch.quantile(weight.view(-1), sparsity/100) # Compute the threshold locally\n",
    "\n",
    "        else: raise NameError('Invalid Method')\n",
    "\n",
    "        if threshold > weight.max(): threshold = weight.max() # Make sure we don't remove every weight of a given layer\n",
    "\n",
    "        mask = weight.ge(threshold).to(dtype=weight.dtype)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0af9e",
   "metadata": {},
   "source": [
    "## Now looking at the criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd66401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastcore.basics import *\n",
    "from fastcore.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4020efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criteria():\n",
    "    def __init__(self, f, needs_init=False, needs_update=False, output_f=None, return_init=False):\n",
    "        store_attr()\n",
    "        assert (needs_init and needs_update)==False, \"The init values will be overwritten by the updating ones.\"\n",
    "\n",
    "    def __call__(self, m, granularity):\n",
    "        if self.needs_update and hasattr(m, '_old_weights') == False:\n",
    "            m.register_buffer(\"_old_weights\", m._init_weights.clone()) # If the previous value of weights is not known, take the initial value\n",
    "\n",
    "        if granularity == 'weight':\n",
    "            wf = self.f(m.weight)\n",
    "            if self.needs_init: wi = self.f(m._init_weights)\n",
    "            elif self.needs_update: wi = self.f(m._old_weights)\n",
    "\n",
    "        elif granularity in granularities:\n",
    "            dim = granularities[granularity]\n",
    "            wf = self.f(m.weight).mean(dim=dim, keepdim=True)\n",
    "            if self.needs_init: wi = self.f(m._init_weights).mean(dim=dim, keepdim=True)\n",
    "            elif self.needs_update: wi = self.f(m._old_weights).mean(dim=dim, keepdim=True)\n",
    "\n",
    "        else: raise NameError('Invalid Granularity')\n",
    "\n",
    "        if self.needs_update: m._old_weights = m.weight.clone() # The current value becomes the old one for the next iteration\n",
    "\n",
    "        if self.output_f: return self.output_f(wf, wi)\n",
    "        elif self.return_init: return wi\n",
    "        else: return wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf83dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hessian Criteria\n",
    "def hess_crit(m, granularity):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf330a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Criteria\n",
    "def grad_crit(m, granularity):\n",
    "    if m.weight.grad is not None:\n",
    "        if granularity == 'weight':\n",
    "            w = (m.weight*m.weight.grad).pow(2)\n",
    "\n",
    "        elif granularity in granularities:\n",
    "            dim = granularities[granularity]\n",
    "            w = (m.weight*m.weight.grad).pow(2).mean(dim=dim, keepdim=True)\n",
    "\n",
    "        else: raise NameError('Invalid Granularity') \n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8398d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_final = Criteria(torch.abs)\n",
    "weight_pruner = Sparsifier(resnet18, 'weight', 'global', large_final)\n",
    "\n",
    "grad_pruner = Sparsifier(resnet18, 'weight', 'global', grad_crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bf8af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7bd32cda2d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./saved/model/ResNet_10.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accuracy before pruning: {accuracy(resnet18, cifar_10_test_dl)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Before Pruning: ResNet18: {count_parameters(resnet18):,} parameters\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Loading Model...\")\n",
    "resnet18 = torch.load(\"./saved/model/ResNet_10.pt\")\n",
    "\n",
    "print(f\"accuracy before pruning: {accuracy(resnet18, cifar_10_test_dl)}\")\n",
    "print(f\"Before Pruning: ResNet18: {count_parameters(resnet18):,} parameters\\n\")\n",
    "\n",
    "print(\"pruning with weight pruner...\\n\")\n",
    "weight_pruner.prune_model(sparsity=80)\n",
    "\n",
    "print(f\"accuracy after pruning: {accuracy(resnet18, cifar_10_test_dl)}\")\n",
    "print(f\"After weight pruning: ResNet18: {count_parameters(resnet18):,} parameters\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading Model...\")\n",
    "resnet18 = torch.load(\"./saved/model/ResNet_10.pt\")\n",
    "\n",
    "print(f\"accuracy before pruning: {accuracy(resnet18, cifar_10_test_dl)}\")\n",
    "print(f\"Before Pruning: ResNet18: {count_parameters(resnet18):,} parameters\\n\")\n",
    "\n",
    "print(\"pruning with gradient pruner...\\n\")\n",
    "grad_pruner.prune_model(sparsity=80)\n",
    "\n",
    "print(f\"accuracy after pruning: {accuracy(resnet18, cifar_10_test_dl)}\")\n",
    "print(f\"After weight pruning: ResNet18: {count_parameters(resnet18):,} parameters\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b26dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1885561",
   "metadata": {},
   "source": [
    "## Viisualizing the Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23cdecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernels(layer, save=None):\n",
    "    kernels = layer.weight.detach().clone()\n",
    "    kernels = kernels - kernels.min()\n",
    "    kernels = kernels/kernels.max()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    img = make_grid(kernels, nrow=8, padding=1, pad_value=1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.detach().permute(1,2,0).cpu())\n",
    "    if save: plt.savefig(f'{save}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7029b7",
   "metadata": {},
   "source": [
    "## Visualizing The Data\n",
    "\n",
    "Code to view 9 random images in the training data, as well as use the Dataloader to ensure everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels_map = {\n",
    "    0: \"plane\",\n",
    "    1: \"car\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(cifar_10_train), size=(1,)).item()\n",
    "    img, label = cifar_10_train[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(cifar_10_train_dl))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "imshow(img)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b00522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m72",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m72"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
